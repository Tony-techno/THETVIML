# -*- coding: utf-8 -*-
"""TechnoHacks_EduTech_fraud_transaction_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqEXrlo7RrIU3dvdqxD6jQDxNyE3qLUP

#**Fraud Transaction Detection**

#**1. Importing Libraries**

###1. numpy: A library for efficient numerical computation.
###2. pandas: A library for data manipulation and analysis.
###3. seaborn: A visualization library based on matplotlib.
###4. matplotlib.pyplot: A plotting library.
###5. RandomOverSampler: A class for handling imbalanced data by oversampling the minority class.
###6. StandardScaler: A scaler that scales features to a common range.
###7. RandomForestClassifier: A classifier that combines multiple decision trees to make predictions.
"""

# Import Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

"""#**2.Data Loading and Preprocessing**

###1. Loading the Dataset: Loads the credit card transaction dataset from a CSV file.
"""

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

"""###2. Data Shape: Displays the shape of the loaded data."""

# Explore the dataset
print(df.shape)

"""###3. Data Information: Displays information about the data, including data types and missing values."""

print(df.columns)

df.info()

"""###4. Data Head and Tail: Displays the first and last few rows of the data."""

df.head()

df.tail()

"""###5. Data Null Values: Checks for null values in the data."""

df.isnull()

"""###6. Data Null Values Sum: Calculates the total number of null values in the data."""

df.isnull().sum()

df.isnull().sum().sum()

"""###7. Data Description: Displays summary statistics for the data."""

df.describe()

df.describe().transpose()

"""###8. Data Handling Missing Values: Handles missing values by filling them with the mean of the column."""

# Handle missing values by filling them with the mean of the column
df.fillna(df.mean(), inplace=True)

df.isnull().sum().sum()

"""#**3. Model Training and Evaluation**"""

# Ensure the 'Class' column is of integer type
df['Class'] = df['Class'].astype(int)

# Split the data into features (X) and target (y)
X = df.drop(['Class'], axis=1)  # features
y = df['Class']  # target variable (fraudulent or not)

# Handle Imbalanced Data
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

"""###1. Splitting Data: Splits the data into training and testing sets."""

## 8. Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create a Random Forest Classifier model
rfc = RandomForestClassifier(n_estimators=100, random_state=42)

"""###2. Training Model: Trains a Random Forest Classifier model on the training data."""

# Train the model on the training data
rfc.fit(X_train_scaled, y_train)

"""###3. Making Predictions: Makes predictions on the testing data."""

# Make predictions on the testing data
y_pred = rfc.predict(X_test_scaled)

"""###4. Model Evaluation Metrics: Calculates the accuracy, precision, recall, and F1-score for the model.
###5. Confusion Matrix: Displays the confusion matrix for the model.
"""

# Evaluate the model
print('*'*50)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print('*'*50)
precision = precision_score(y_test, y_pred)
print(f"Precision: {precision:.2f}")
print('*'*50)
recall = recall_score(y_test, y_pred)
print(f"Recall: {recall:.2f}")
print('*'*50)
f1 = f1_score(y_test, y_pred)
print(f"F1-score: {f1:.2f}")
print('*'*50)
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print('*'*50)

# Perform Cross-Validation
scores = cross_val_score(rfc, X_resampled, y_resampled, cv=5)
print(f"Cross-Validation Accuracy: {scores.mean():.2f} Â± {scores.std():.2f}")

"""#**4. Using the Model to Detect Fraudulent Transactions**

###1. Function to Detect Fraud: Defines a function to detect fraudulent transactions given input data.
"""

# Use the model to detect fraudulent transactions
def detect_fraud(transaction_data):
    # Preprocess the transaction data (e.g., scale/normalize)
    transaction_data = pd.DataFrame(transaction_data, columns=X.columns)
    transaction_data_scaled = scaler.transform(transaction_data)
    # Make predictions
    predictions = rfc.predict(transaction_data_scaled)
    return ["Yes" if pred else "No" for pred in predictions.tolist()]

"""###2. Example Usage: Uses the function to detect fraudulent transactions for the entire dataset."""

print(detect_fraud(df))

"""###3. Multiple Examples: Detects fraudulent transactions for multiple sets of data."""

print("Is there fraud? " + detect_fraud(df)[0])

print("Is there fraud? " + detect_fraud(df.head())[0])

print("Is there fraud? " + detect_fraud(df.tail())[0])

"""#**5. Summary**

###This code demonstrates the steps involved in building a Random Forest Classifier model to detect fraudulent transactions in a credit card dataset. It includes data loading and preprocessing, model training and evaluation, and using the model to detect fraudulent transactions. The model is trained on the training data and evaluated using various metrics such as accuracy, precision, recall, and F1-score. The model is then used to detect fraudulent transactions in the testing data and in multiple sets of data.
"""