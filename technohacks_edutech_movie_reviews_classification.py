# -*- coding: utf-8 -*-
"""TechnoHacks_EduTech_movie_reviews_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cki9WkaFCzG8AKNPidjFVqtksaClfYmp

#**Movie Review Classification**

#**1. Importing Libraries**

###1. nltk: A natural language processing library for tokenizing, stemming, and lemmatizing text.
###2. pandas: A powerful and flexible data analysis and manipulation library.
###3. TfidfVectorizer: A library for converting text data into a numerical format using TF-IDF vectorization.
###4. MultinomialNB: A Naive Bayes classifier for text classification.
###5. FunctionTransformer: A library for transforming functions in a pipeline.
###6. Pipeline: A library for creating and managing pipelines of data processing steps.
###7. requests: A library for making HTTP requests and interacting with REST APIs.
###8. Beautiful Soup: A library for parsing and scraping HTML and XML data.
###9. WordNetLemmatizer: A library for lemmatizing words based on their part-of-speech tags.
###10. stopwords: A library for removing common stop words from text data.
"""

!pip install nltk

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

nltk.download('punkt')
print(nltk.data.path)

nltk.download('wordnet')
print(nltk.data.path)

nltk.download('stopwords')
print(nltk.data.path)

"""#**2. Data Loading and Preprocessing**

###1. Loading Data: This section loads the data from various CSV files.
"""

# Load the data
df1 = pd.read_csv('/content/sampleSubmission.csv')
df2 = pd.read_csv('/content/labeledTrainData.tsv', sep='\t')
df3 = pd.DataFrame([line.strip().split('\t') for line in open('/content/unlabeledTrainData.tsv', 'r')])
df4 = pd.read_csv('/content/testData.tsv', sep='\t')

"""###2. Data Shape: Displays the shape of the loaded data."""

df1.shape

df2.shape

df3.shape

df4.shape

"""###3. Data Information: Displays information about the data, including data types and missing values."""

df1.info()

df2.info()

df3.info()

df4.info()

"""###4. Data Head and Tail: Displays the first and last few rows of the data."""

df1.head()

df1.tail()

df2.head()

df2.tail()

df3.head()

df3.tail()

df4.head()

df4.tail()

"""###5. Data Null Values: Checks for null values in the data.
###6. Data Null Values Sum: Calculates the total number of null values in the data.
"""

df1.isnull()

df1.isnull().sum()

df1.isnull().sum().sum()

df2.isnull()

df2.isnull().sum()

df2.isnull().sum().sum()

df3.isnull()

df3.isnull().sum()

df3.isnull().sum().sum()

df4.isnull()

df4.isnull().sum()

df4.isnull().sum().sum()

"""###7. Data Description: Displays summary statistics for the data."""

df1.describe()

df1.describe().transpose()

df2.describe()

df2.describe().transpose()

df3.describe()

df3.describe().transpose()

df4.describe()

df4.describe().transpose()

"""#**3. Preprocessing**"""

# Combine the labeled and unlabeled data
df = pd.concat([df2, df3])

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
print(df4.isnull().sum())

df['review'] = df['review'].fillna('')

"""###1. Preprocessing Function: Defines a function to preprocess the text data by tokenizing, removing stop words, and lemmatizing."""

# Define the preprocessing function
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]

    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]

    # Join the tokens back into a string
    return ' '.join(tokens)

"""###2. Text Vectorization: Converts the text data into a numerical format using TF-IDF vectorization."""

# Convert the text data into a numerical format
vectorizer = TfidfVectorizer(preprocessor=preprocess_text)

"""#**4. Model Training and Evaluation**

###1. Splitting Data: Splits the data into training and validation sets.
"""

# Split the data into training and validation sets
X = df['review']
y = df['sentiment']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""###2. Training Model: Trains a Naive Bayes classifier on the training data."""

y_train = y_train.fillna(y_train.mean())

print(y_train.dtype)

y_train = y_train.astype(int)

# Create the pipeline
pipeline = Pipeline([
    ('vectorizer', vectorizer),
    ('classifier', MultinomialNB())
])

# Train the model
pipeline.fit(X_train, y_train)

"""###3. Model Evaluation: Evaluates the model using accuracy, classification report, and confusion matrix."""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
print(y_val.isnull().sum())

"""#**5. Predicting**

###1. Making Predictions: Makes predictions on the test data.
"""

# Make predictions on the test data
y_pred = pipeline.predict(df4['review'])

"""###2. Creating Submission File: Creates a submission file with predicted sentiment values."""

# Create the submission file
submission = pd.DataFrame({'id': df4['id'], 'sentiment': y_pred})
submission.to_csv('submission.csv', index=False)

"""#**6. Summary**

###This code classifies movie reviews as positive or negative using natural language processing (NLP) techniques. It loads movie review data, preprocesses the text data, and converts it into a numerical format using TF-IDF vectorization. Then, it trains a Naive Bayes classifier on the preprocessed data. Finally, it makes predictions on the test data and generates a submission file with predicted sentiment values.
"""